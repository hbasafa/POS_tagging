{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pos.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hbasafa/POS_tagging/blob/master/pos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SBZZz0CWIBj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "1b367afb-90e8-473c-8f8a-945ff7554108"
      },
      "source": [
        "from time import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hazm\n",
        "from keras import Sequential\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.engine import InputLayer\n",
        "from keras.layers import Embedding, Dense, Bidirectional, LSTM, TimeDistributed, Activation, K\n",
        "from keras.optimizers import Adam\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import zipfile\n",
        "\n",
        "\n",
        "# path_to_zip_file = \"/content/CA4_data.zip\"\n",
        "# zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\n",
        "# directory_to_extract_to = \"/content\"\n",
        "# zip_ref.extractall(directory_to_extract_to)\n",
        "# zip_ref.close()\n",
        "\n",
        "train_path = \"/content/train.txt\"\n",
        "\n",
        "\n",
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)\n",
        "\n",
        "\n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        "\n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "\n",
        "    return ignore_accuracy\n",
        "\n",
        "\n",
        "def generate_sample(X_data, y_data, batch_size):\n",
        "\n",
        "  samples_per_epoch = X_data.shape[0]\n",
        "  number_of_batches = samples_per_epoch/batch_size\n",
        "  counter=0\n",
        "\n",
        "  while 1:\n",
        "\n",
        "    X_batch = np.array(X_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "    y_batch = np.array(y_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "    counter += 1\n",
        "    yield X_batch,y_batch\n",
        "\n",
        "    #restart counter to yeild data in the next epoch as well\n",
        "    if counter >= number_of_batches:\n",
        "        counter = 0\n",
        "\n",
        "\n",
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        "\n",
        "        token_sequences.append(token_sequence)\n",
        "\n",
        "    return token_sequences\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # preparing data\n",
        "    train_data = pd.read_csv(train_path, '\\s{2,}')\n",
        "    words = train_data[\"#\"]\n",
        "    tags = train_data[\"DELM\"]\n",
        "\n",
        "    # cropping data\n",
        "    r = -1\n",
        "    words = words[:r]\n",
        "    tags = tags[:r]\n",
        "\n",
        "    dummy_word = \"Jafar\"\n",
        "    text = (' ' + dummy_word + ' ').join(words)\n",
        "    sentences = hazm.sent_tokenize(text)\n",
        "    X, y = [], []\n",
        "    k = 0\n",
        "    for sent in sentences:\n",
        "        s_words = sent.split(dummy_word)\n",
        "        n = len(s_words)\n",
        "        X.append(words[k:k + n])\n",
        "        y.append(tags[k:k + n])\n",
        "        k += n\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    words, tags = set(words), set(tags)\n",
        "\n",
        "    n_split = 5\n",
        "    kf = KFold(n_splits=n_split, shuffle=True)\n",
        "    # (train_sentences,\n",
        "    #  test_sentences,\n",
        "    #  train_tags,\n",
        "    #  test_tags) = train_test_split(X, y, test_size=0.2)\n",
        "    total_scores = []\n",
        "    \n",
        "    for train_index, test_index in kf.split(X=X, y=y):\n",
        "        train_sentences, test_sentences = X[train_index], X[test_index]\n",
        "        train_tags, test_tags = y[train_index], y[test_index]\n",
        "        # assigning number\n",
        "        word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "        word2index['-PAD-'] = 0  # The special value used for padding\n",
        "        word2index['-OOV-'] = 1  # The special value used for OOVs\n",
        "\n",
        "        tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
        "        tag2index['-PAD-'] = 0  # The special value used to padding\n",
        "\n",
        "        train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
        "\n",
        "        # preparing inputs\n",
        "        for s in train_sentences:\n",
        "            s_int = []\n",
        "            for w in s:\n",
        "                try:\n",
        "                    s_int.append(word2index[w])\n",
        "                except KeyError:\n",
        "                    s_int.append(word2index['-OOV-'])\n",
        "\n",
        "            train_sentences_X.append(s_int)\n",
        "\n",
        "        for s in test_sentences:\n",
        "            s_int = []\n",
        "            for w in s:\n",
        "                try:\n",
        "                    s_int.append(word2index[w])\n",
        "                except KeyError:\n",
        "                    s_int.append(word2index['-OOV-'])\n",
        "\n",
        "            test_sentences_X.append(s_int)\n",
        "\n",
        "        for s in train_tags:\n",
        "            train_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "        for s in test_tags:\n",
        "            test_tags_y.append([tag2index[t] for t in s])\n",
        "\n",
        "        MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "        print(MAX_LENGTH)\n",
        "\n",
        "        train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "        test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "        train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "        test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(InputLayer(input_shape=(MAX_LENGTH,)))\n",
        "        model.add(Embedding(len(word2index), 64))\n",
        "        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "        model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "        model.add(Activation('softmax'))\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=Adam(0.01),\n",
        "                      metrics=['accuracy', ignore_class_accuracy(0)])\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "\n",
        "        # tensorboard = TensorBoard(log_dir='./../../logs/POS-RNN-demo-2-' + '-{}'.format(time()))\n",
        "\n",
        "        model.fit(train_sentences_X, cat_train_tags_y,\n",
        "                  batch_size=128, epochs=8,\n",
        "                  validation_split=0.2)\n",
        "\n",
        "        # model.save('model_-pos-rnn-demo-2-' + '.h5')\n",
        "\n",
        "        # model.load_weights('model_-pos-rnn-demo-2-.h5')\n",
        "\n",
        "        scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
        "        total_scores.append(scores)\n",
        "        for i in range(len(scores)):\n",
        "            print(f\"{model.metrics_names[i]}: {scores[i] * 100}\")\n",
        "\n",
        "        # predictions = model.predict(test_sentences_X)\n",
        "        # print(list(test_tags[1]))\n",
        "        # print(\"\")\n",
        "        # # m = min(len(predictions), len(to_categorical(test_tags_y, len(tag2index))))-2\n",
        "        # print(ignore_class_accuracy(0)(predictions, to_categorical(test_tags_y, len(tag2index))))\n",
        "        # print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()})[1])\n",
        "\n",
        "    print(f\"total accuracy over 5-fold: {np.mean(total_scores, axis=0)[1] * 100}\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "733\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 733, 64)           3989376   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 733, 256)          197632    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 733, 40)           10280     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 733, 40)           0         \n",
            "=================================================================\n",
            "Total params: 4,197,288\n",
            "Trainable params: 4,197,288\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}